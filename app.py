import streamlit as st
import numpy as np
import librosa
import av
import queue
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration, AudioProcessorBase

st.title("ğŸ¶ é…”ã£æ‰•ã„åº¦åˆ¤å®šã‚¢ãƒ—ãƒªï¼ˆWebRTCï¼‹TURNå¯¾å¿œç‰ˆï¼‰")

RTC_CONFIGURATION = RTCConfiguration({
    "iceServers": [{
        "urls": st.secrets["webrtc"]["turn_uri"],
        "username": st.secrets["webrtc"]["turn_username"],
        "credential": st.secrets["webrtc"]["turn_password"],
    }]
})

# ã‚»ãƒƒã‚·ãƒ§ãƒ³è·¨ãã§ä¿æŒ
if "audio_q" not in st.session_state:
    st.session_state.audio_q = queue.Queue()
if "last_analyzed_bytes" not in st.session_state:
    st.session_state.last_analyzed_bytes = 0

class AudioProcessor(AudioProcessorBase):
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        pcm = frame.to_ndarray().astype(np.float32)  # (channels, samples) or (samples,)
        st.session_state.audio_q.put(pcm)
        return frame

st.caption("1) START â†’ 5ã€œ10ç§’è©±ã™ â†’ 2) STOP â†’ 3) ä¸‹ã®ã€è§£æã€ã‚’æŠ¼ã™")

webrtc_ctx = webrtc_streamer(
    key="audio",
    mode=WebRtcMode.SENDONLY,
    rtc_configuration=RTC_CONFIGURATION,
    media_stream_constraints={"audio": True, "video": False},
    audio_processor_factory=AudioProcessor,
)

# ã‚­ãƒ¥ãƒ¼ã®çŠ¶æ…‹ã‚’è¦‹ãˆã‚‹åŒ–ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
q_size = st.session_state.audio_q.qsize()
st.info(f"å—ä¿¡ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼ˆç›®å®‰ï¼‰: {q_size}  â€»0ã®ã¾ã¾ãªã‚‰ãƒã‚¤ã‚¯ãŒå–ã‚Œã¦ã„ã¾ã›ã‚“")

def drain_audio_queue():
    """ã‚­ãƒ¥ãƒ¼ã‚’å…¨éƒ¨å–ã‚Šå‡ºã—ã¦1æœ¬ã®æ³¢å½¢ã«ã™ã‚‹"""
    chunks = []
    while not st.session_state.audio_q.empty():
        chunks.append(st.session_state.audio_q.get())

    if not chunks:
        return None

    audio = np.concatenate([c.flatten() for c in chunks]).astype(np.float32)
    return audio

# STOPã‚’æ¤œçŸ¥ã™ã‚‹ã ã‘ã ã¨ç’°å¢ƒå·®ãŒã‚ã‚‹ã®ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œã§ç¢ºå®Ÿã«è§£æã•ã›ã‚‹
analyze = st.button("è§£æã™ã‚‹ï¼ˆSTOPå¾Œã«æŠ¼ã™ï¼‰", type="primary")

if analyze:
    audio = drain_audio_queue()
    if audio is None:
        st.error("éŸ³å£°ãŒå–å¾—ã§ãã¦ã„ã¾ã›ã‚“ã€‚STARTå¾Œã«ãƒã‚¤ã‚¯è¨±å¯ãŒå‡ºã¦ã„ã‚‹ã‹ã€åˆ¥ã®ãƒã‚¤ã‚¯ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚")
        st.stop()

    sr = 48000  # streamlit-webrtcã®æ—¢å®šãŒ48kHzã®ã“ã¨ãŒå¤šã„
    audio = audio / (np.max(np.abs(audio)) + 1e-9)

    # 3ç§’æœªæº€ã ã¨ç‰¹å¾´é‡ãŒå®‰å®šã—ãªã„ã®ã§å¼¾ã
    duration = len(audio) / sr
    st.write(f"å–å¾—éŸ³å£°é•·: {duration:.2f} ç§’")
    if duration < 3.0:
        st.warning("éŸ³å£°ãŒçŸ­ã™ãã¾ã™ã€‚3ç§’ä»¥ä¸Šè©±ã—ã¦ã‹ã‚‰STOPâ†’è§£æã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    rms = float(librosa.feature.rms(y=audio).mean())
    zcr = float(librosa.feature.zero_crossing_rate(audio).mean())
    centroid = float(librosa.feature.spectral_centroid(y=audio, sr=sr).mean())

    score = (
        min(rms / 0.2, 1.0) * 0.4 +
        (1 - min(zcr / 0.15, 1.0)) * 0.3 +
        (1 - min(centroid / 4000, 1.0)) * 0.3
    )
    drunk_score = int(max(0, min(100, round(score * 100))))

    st.subheader("ğŸ¶ æ¨å®šé…”ã£æ‰•ã„åº¦ï¼ˆ0ã€œ100ï¼‰")
    st.metric("ã‚¹ã‚³ã‚¢", drunk_score)

    with st.expander("è§£æè©³ç´°"):
        st.json({
            "rms": rms,
            "zcr": zcr,
            "centroid": centroid,
            "duration_sec": duration,
        })
