# app.py
# -*- coding: utf-8 -*-
"""
é…”ã£æ‰•ã„åº¦ï¼ˆ0-100ï¼‰ã‚’æ¨å®šã™ã‚‹ãƒ‡ãƒ¢ã‚¢ãƒ—ãƒªã€‚
- è¡¨ç¤ºã—ãŸæ—¥æœ¬èªæ–‡ç« ã‚’èª­ã¿ä¸Šã’ã¦ã‚‚ã‚‰ã„ã€ã‚¹ãƒãƒ›/PCã®ãƒã‚¤ã‚¯ã‚’WebRTCçµŒç”±ã§å–å¾—
- éŸ³é‡ï¼ˆRMSï¼‰ã¨ç™ºè©±ã®æ˜ç­åº¦ï¼ˆã‚ã‚Œã¤ï¼‰ã«é–¢ä¿‚ã™ã‚‹ç‰¹å¾´é‡ã‹ã‚‰ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
- ä»»æ„ã§ã€Œå¹³å¸¸æ™‚ã®è‡ªåˆ†ã€ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã—ã¦ä¿å­˜

å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª:
    pip install streamlit streamlit-webrtc av numpy librosa soundfile scipy

èµ·å‹•:
    streamlit run app.py

æ³¨æ„:
    * ã“ã‚Œã¯å®Ÿé¨“ç”¨ãƒ‡ãƒ¢ã€‚åŒ»ç™‚ãƒ»æ³•çš„åˆ¤å®šç”¨é€”ã§ã¯ä½¿ã‚ãªã„ã§ãã ã•ã„ã€‚
    * å®Ÿãƒ‡ãƒã‚¤ã‚¹ãƒ»ç’°å¢ƒå·®ãŒå¤§ãã„ãŸã‚ã€ã‚¹ã‚³ã‚¢ã¯ç›¸å¯¾çš„ãªæŒ‡æ¨™ã§ã™ã€‚
"""

import av
import io
import math
import queue
import threading
from dataclasses import dataclass
from typing import List, Tuple, Dict, Optional

import numpy as np
import streamlit as st
from streamlit_webrtc import WebRtcMode, webrtc_streamer, RTCConfiguration
import soundfile as sf
import librosa
from scipy.signal import medfilt

st.set_page_config(page_title="é…”ã£æ‰•ã„åº¦åˆ¤å®šãƒ‡ãƒ¢", page_icon="ğŸ¶", layout="centered")

RTC_CONFIGURATION = RTCConfiguration({
    "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
})

# ----------------------------- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ----------------------------- #

def rms_dbfs(y: np.ndarray) -> float:
    eps = 1e-9
    return 20.0 * np.log10(np.sqrt(np.mean(np.square(y)) + eps))


def vad_mask(y: np.ndarray, sr: int, frame_ms: float = 30.0, hop_ms: float = 10.0,
             energy_thresh_db: float = -45.0) -> np.ndarray:
    """éå¸¸ã«å˜ç´”ãªã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹VADã€‚True=æœ‰å£°ã€‚
    energy_thresh_db ã¯RMS[dBFS]ã®ã—ãã„å€¤ã€‚
    """
    frame_len = int(sr * frame_ms / 1000)
    hop_len = int(sr * hop_ms / 1000)
    rms_list = []
    for i in range(0, len(y) - frame_len, hop_len):
        frame = y[i:i+frame_len]
        rms_list.append(rms_dbfs(frame))
    rms_arr = np.array(rms_list)
    mask = rms_arr > energy_thresh_db
    return mask


def syllable_like_rate(y: np.ndarray, sr: int) -> float:
    """ç™ºè©±é€Ÿåº¦ã®ç°¡æ˜“æ¨å®šï¼ˆã‚·ãƒ©ãƒ–ãƒ«/ç§’ã«ç›¸å½“ï¼‰ã€‚
    ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ•ãƒ©ãƒƒã‚¯ã‚¹ã®ãƒ”ãƒ¼ã‚¯æ•°ã‚’æ•°ãˆã‚‹ç°¡æ˜“æ‰‹æ³•ã€‚
    """
    S = np.abs(librosa.stft(y, n_fft=1024, hop_length=256))
    flux = np.diff(S, axis=1)
    flux = np.maximum(flux, 0).mean(axis=0)
    flux = (flux - np.min(flux)) / (np.max(flux) - np.min(flux) + 1e-9)
    flux_smooth = medfilt(flux, kernel_size=7)
    # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºï¼ˆã—ãã„å€¤è¶…ãˆï¼†å±€æ‰€æœ€å¤§ï¼‰
    thr = 0.6
    peaks = []
    for i in range(1, len(flux_smooth)-1):
        if flux_smooth[i] > thr and flux_smooth[i] > flux_smooth[i-1] and flux_smooth[i] > flux_smooth[i+1]:
            peaks.append(i)
    time = np.arange(len(flux_smooth)) * (256 / sr)
    duration = time[-1] if len(time) > 0 else 0.0
    rate = (len(peaks) / duration) if duration > 0 else 0.0
    return float(rate)


def slur_features(y: np.ndarray, sr: int) -> Dict[str, float]:
    """ã‚ã‚Œã¤ã«é–¢é€£ã—ãã†ãªç°¡æ˜“ã‚¹ãƒšã‚¯ãƒˆãƒ«ç‰¹å¾´ã‚’è¨ˆç®—ã€‚
    é«˜ã‚¹ãƒ«ãƒ¼ã¯ä¸€èˆ¬ã«æ˜ç­åº¦â†“: ã‚¹ãƒšã‚¯ãƒˆãƒ«å¹³å¦åº¦â†‘, ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰â†“, ZCRâ†“ ãªã©ã‚’ä»®å®šã€‚
    """
    y = librosa.util.normalize(y)
    S = np.abs(librosa.stft(y, n_fft=1024, hop_length=256)) + 1e-9
    cent = librosa.feature.spectral_centroid(S=S, sr=sr).flatten()
    flat = librosa.feature.spectral_flatness(S=S).flatten()
    zcr = librosa.feature.zero_crossing_rate(y, frame_length=1024, hop_length=256).flatten()

    features = {
        "centroid_mean": float(np.mean(cent)),
        "centroid_std": float(np.std(cent)),
        "flatness_mean": float(np.mean(flat)),
        "flatness_std": float(np.std(flat)),
        "zcr_mean": float(np.mean(zcr)),
        "zcr_std": float(np.std(zcr)),
    }
    return features


def normalize_feature(x: float, lo: float, hi: float, invert: bool = False) -> float:
    x_clamped = max(lo, min(hi, x))
    norm = (x_clamped - lo) / (hi - lo + 1e-9)
    return 1.0 - norm if invert else norm


@dataclass
class AnalysisResult:
    rms_db: float
    voiced_ratio: float
    speech_rate: float
    slur_score_raw: float
    drunkness: int
    details: Dict[str, float]


# --------------------------- ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° ãƒ­ã‚¸ãƒƒã‚¯ --------------------------- #

def score_drunkness(y: np.ndarray, sr: int, baseline: Optional[Dict[str, float]] = None) -> AnalysisResult:
    # å…¨ä½“éŸ³é‡
    rms = rms_dbfs(y)

    # VAD
    mask = vad_mask(y, sr)
    voiced_ratio = float(mask.mean()) if mask.size > 0 else 0.0

    # ç™ºè©±é€Ÿåº¦ï¼ˆç–‘ä¼¼ã‚·ãƒ©ãƒ–ãƒ«/ç§’ï¼‰
    rate = syllable_like_rate(y, sr)

    # ã‚ã‚Œã¤é–¢é€£ç‰¹å¾´
    f = slur_features(y, sr)

    # -------- æ­£è¦åŒ–ï¼ˆçµŒé¨“çš„ãƒ¬ãƒ³ã‚¸ã€‚ç«¯æœ«å·®å¸åã®ãŸã‚åºƒã‚ã«ï¼‰ -------- #
    loud_norm = normalize_feature(rms, -45.0, -10.0, invert=False)  # å¤§ãã„ã»ã©é…”ã„ãƒã‚¤ãƒ³ãƒˆâ†‘
    rate_norm = normalize_feature(rate, 1.5, 6.0, invert=True)      # é…ã„ã»ã©é…”ã„ãƒã‚¤ãƒ³ãƒˆâ†‘
    voiced_norm = normalize_feature(voiced_ratio, 0.3, 0.95, invert=True)  # æ–­ç¶šçš„/æ²ˆé»™å¤šã„ã»ã©â†‘
    flat_norm = normalize_feature(f["flatness_mean"], 0.05, 0.5, invert=False) # å¹³å¦åº¦é«˜ã„ã»ã©â†‘
    cent_norm = normalize_feature(f["centroid_mean"], 1500, 4500, invert=True)  # ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ä½ã„ã»ã©â†‘
    zcr_norm = normalize_feature(f["zcr_mean"], 0.02, 0.12, invert=True)        # ZCRä½ã„ã»ã©â†‘

    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è£œæ­£ï¼ˆä»»æ„ï¼‰
    baseline_boost = 0.0
    if baseline:
        # è‡ªå·±å¯¾æ¯”: å¹³å¸¸æ™‚ã‚ˆã‚ŠéŸ³é‡ãŒã©ã‚Œã ã‘å¤§ãã„ã‹ã€é€Ÿåº¦ãŒã©ã‚Œã ã‘é…ã„ã‹
        loud_delta = max(0.0, (rms - baseline.get("rms_db", rms)) / 10.0)  # 10dBã§+1
        rate_delta = max(0.0, (baseline.get("rate", rate) - rate) / 2.0)   # 2ã‚·ãƒ©ãƒ–ãƒ«/ç§’ã§+1
        baseline_boost = 10.0 * (loud_delta + rate_delta)

    # é‡ã¿ä»˜ã‘ï¼ˆåˆè¨ˆ100ã«è¿‘ã¥ãã‚ˆã†èª¿æ•´ï¼‰
    score = (
        30.0 * loud_norm +
        20.0 * (0.5*rate_norm + 0.5*voiced_norm) +
        40.0 * (0.5*flat_norm + 0.3*cent_norm + 0.2*zcr_norm) +
        baseline_boost
    )
    score = int(max(0, min(100, round(score))))

    details = {
        "rms_db": rms,
        "voiced_ratio": voiced_ratio,
        "speech_rate": rate,
        **f,
        "loud_norm": loud_norm,
        "rate_norm": rate_norm,
        "voiced_norm": voiced_norm,
        "flat_norm": flat_norm,
        "centroid_norm": cent_norm,
        "zcr_norm": zcr_norm,
        "baseline_boost": baseline_boost,
    }

    return AnalysisResult(rms, voiced_ratio, rate, (flat_norm+cent_norm+zcr_norm)/3.0, score, details)


# ------------------------------ UI / WebRTC ------------------------------ #

st.title("ğŸ¶ é…”ã£æ‰•ã„åº¦åˆ¤å®šãƒ‡ãƒ¢")

st.markdown(
    """
**æ‰‹é †**
1. ä¸‹ã®æ–‡ç« ã‚’å£°ã«å‡ºã—ã¦èª­ã¿ä¸Šã’ã¦ãã ã•ã„ï¼ˆã§ãã‚Œã°ä¸€å®šã®é€Ÿã•ãƒ»å£°é‡ã§ï¼‰
2. ã€ŒéŒ²éŸ³é–‹å§‹ã€ã‚’æŠ¼ã—ã¦10ç§’ç¨‹åº¦éŒ²éŸ³ â†’ ã€Œåœæ­¢ã€ã§è§£æ
3. å¿…è¦ãªã‚‰ã€Œå¹³å¸¸æ™‚ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¿å­˜ã€ã§è‡ªåˆ†å°‚ç”¨ã®è£œæ­£ã‚’æœ‰åŠ¹åŒ–

> âš ï¸ ãƒ‡ãƒ¢ç”¨é€”ã§ã™ã€‚ç«¯æœ«ã‚„ç’°å¢ƒã€å€‹äººå·®ã«ã‚ˆã£ã¦ã°ã‚‰ã¤ãã¾ã™ã€‚
    """
)

TEXTS = [
    "ç”Ÿéº¦ç”Ÿç±³ç”Ÿåµã€‚éš£ã®å®¢ã¯ã‚ˆãæŸ¿é£Ÿã†å®¢ã ã€‚",
    "èµ¤å·»ç´™é’å·»ç´™é»„å·»ç´™ã€‚é›¨ãŒã‚ãŒã‚Œã°ç¶¾ãªã™å½©ã€‚",
    "æ±äº¬ç‰¹è¨±è¨±å¯å±€ã€ä»Šæ—¥æ€¥é½è¨±å¯å´ä¸‹ã€‚",
]

text_choice = st.selectbox("èª­ã¿ä¸Šã’ã‚‹æ–‡ç« ", TEXTS, index=0)
st.info(text_choice)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ
if "audio_buffers" not in st.session_state:
    st.session_state.audio_buffers = []
if "baseline" not in st.session_state:
    st.session_state.baseline = None


class AudioRecorder:
    def __init__(self):
        self.q: "queue.Queue[av.AudioFrame]" = queue.Queue()
        self.frames: List[np.ndarray] = []
        self.sr: int = 48000  # WebRTCæ¨™æº–
        self.channels: int = 1

    def recv_callback(self, frame: av.AudioFrame) -> av.AudioFrame:
        # ãƒ¢ãƒãƒ©ãƒ«ãƒ»float32ã¸
        frame = frame.to_ndarray(format="s16", layout="mono")
        # int16 -> float32
        pcm = frame.astype(np.float32) / 32768.0
        self.frames.append(pcm.copy())
        return av.AudioFrame.from_ndarray((pcm * 32768.0).astype(np.int16), format="s16", layout="mono")

    def get_audio(self) -> Tuple[np.ndarray, int]:
        if not self.frames:
            return np.zeros(0, dtype=np.float32), self.sr
        y = np.concatenate(self.frames)
        return y, self.sr


recorder = AudioRecorder()

col1, col2, col3 = st.columns(3)
with col1:
    duration_sec = st.slider("éŒ²éŸ³é•·(ç§’)", 5, 20, 10, 1)
with col2:
    st.write("")
with col3:
    energy_thresh = st.slider("VADã—ãã„å€¤(dBFS)", -70, -20, -45, 1)

webrtc_ctx = webrtc_streamer(
    key="speech-capture",
    mode=WebRtcMode.SENDONLY,
    audio_receiver_size=256,
    media_stream_constraints={"audio": True, "video": False},
    rtc_configuration=RTC_CONFIGURATION,
    async_processing=False,
)

if webrtc_ctx.state.playing:
    webrtc_ctx.receiver.audio_transformer = recorder.recv_callback

st.markdown("---")

# éŒ²éŸ³ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
start = st.button("âºï¸ éŒ²éŸ³é–‹å§‹")
stop = st.button("â¹ï¸ åœæ­¢ãƒ»è§£æ")

if start and webrtc_ctx.state.playing:
    recorder.frames = []
    st.session_state.audio_buffers = []
    st.info("éŒ²éŸ³ä¸­â€¦ æŒ‡å®šç§’æ•°èª­ã‚“ã ã‚‰ã€åœæ­¢ãƒ»è§£æã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

if stop:
    y, sr = recorder.get_audio()
    if y.size == 0:
        st.warning("éŸ³å£°ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒã‚¤ã‚¯æ¨©é™ã‚„æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    else:
        # å¿…è¦ãªã‚‰ãƒˆãƒªãƒ ï¼ˆç„¡éŸ³ã‚«ãƒƒãƒˆï¼‰
        yt, _ = librosa.effects.trim(y, top_db=40)
        if yt.size < sr * 0.8:
            yt = y  # éåº¦ã«åˆ‡ã‚ŒãŸã‚‰å…ƒã«æˆ»ã™

        # è§£æ
        res = score_drunkness(yt, sr, baseline=st.session_state.baseline)

        # ä¿å­˜WAVï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        buf = io.BytesIO()
        sf.write(buf, yt, sr, format="WAV")
        st.session_state.audio_buffers.append(buf.getvalue())

        st.subheader("æ¨å®šçµæœ")
        st.metric("ğŸ¶ é…”ã£æ‰•ã„åº¦", f"{res.drunkness}/100")

        with st.expander("è©³ç´°æŒ‡æ¨™"):
            st.json(res.details)

        st.download_button("è§£æéŸ³å£°ã‚’WAVã§ä¿å­˜", data=st.session_state.audio_buffers[-1], file_name="sample.wav", mime="audio/wav")

        st.caption("*ã‚¹ã‚³ã‚¢ã¯ç›¸å¯¾æŒ‡æ¨™ã§ã™ã€‚æ˜ç­ãªç™ºè©±ã»ã©ä½ã‚¹ã‚³ã‚¢ã€å£°é‡ãŒå¤§ããä¸æ˜ç­ãªã»ã©é«˜ã‚¹ã‚³ã‚¢ã«ãªã‚Šã‚„ã™ã„è¨­è¨ˆã§ã™ã€‚*")

st.markdown("---")

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆå¹³å¸¸æ™‚ï¼‰ã‚µãƒ³ãƒ—ãƒ«ã®ä¿å­˜
st.subheader("ä»»æ„: å¹³å¸¸æ™‚ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¿å­˜ã—ã¦è£œæ­£ã™ã‚‹")
if st.button("å¹³å¸¸æ™‚ã‚µãƒ³ãƒ—ãƒ«ã‚’ç¾åœ¨éŸ³å£°ã‹ã‚‰ä¿å­˜"):
    y, sr = recorder.get_audio()
    if y.size == 0:
        st.warning("ã¾ãšéŒ²éŸ³ã—ã¦ãã ã•ã„ã€‚")
    else:
        yt, _ = librosa.effects.trim(y, top_db=40)
        f = slur_features(yt, sr)
        base = {
            "rms_db": rms_dbfs(yt),
            "rate": syllable_like_rate(yt, sr),
            "flat": f["flatness_mean"],
            "cent": f["centroid_mean"],
            "zcr": f["zcr_mean"],
        }
        st.session_state.baseline = base
        st.success("å¹³å¸¸æ™‚ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚ã“ã®å¾Œã®åˆ¤å®šã«è‡ªå·±å¯¾æ¯”è£œæ­£ã‚’åŠ ãˆã¾ã™ã€‚")
        st.json(base)

st.caption("Â© Demo. This is a heuristic prototype; not a diagnostic tool.")
